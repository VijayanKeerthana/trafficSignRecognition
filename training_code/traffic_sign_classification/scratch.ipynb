{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in d:\\anaconda\\lib\\site-packages (4.5.4.60)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda\\lib\\site-packages (from opencv-contrib-python) (1.19.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scikit-image in d:\\anaconda\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.15.1 in d:\\anaconda\\lib\\site-packages (from scikit-image) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from scikit-image) (1.5.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-image) (3.3.2)\n",
      "Requirement already satisfied: networkx>=2.0 in d:\\anaconda\\lib\\site-packages (from scikit-image) (2.5)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in d:\\anaconda\\lib\\site-packages (from scikit-image) (8.0.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in d:\\anaconda\\lib\\site-packages (from scikit-image) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in d:\\anaconda\\lib\\site-packages (from scikit-image) (2020.10.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in d:\\anaconda\\lib\\site-packages (from scikit-image) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in d:\\anaconda\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\anaconda\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\anaconda\\lib\\site-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
      "Requirement already satisfied: imutils in d:\\anaconda\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in d:\\anaconda\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: libclang>=9.0.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in d:\\anaconda\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in d:\\anaconda\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "# workon traffic_signs\n",
    "!pip install opencv-contrib-python\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-image\n",
    "!pip install imutils\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = os.getcwd()\n",
    "save_path = \"./trafficsignnet.model\"\n",
    "plot_path = \"./plot.png\"\n",
    "test_path = \"./Test/\"\n",
    "example_path = \"./examples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "class TrafficSignNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        \n",
    "        # CONV => RELU => BN => POOL\n",
    "        model.add(Conv2D(8, (5, 5), padding=\"same\",input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # first set of (CONV => RELU => CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                  \n",
    "        # second set of (CONV => RELU => CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                  \n",
    "        # first set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "                  \n",
    "        # second set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "                  \n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "                  \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "# from pyimagesearch.trafficsignnet import TrafficSignNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(basePath, csvPath):\n",
    "    # initialize the list of data and labels\n",
    "    data = []\n",
    "    labels = []\n",
    "    # load the contents of the CSV file, remove the first line (since\n",
    "    # it contains the CSV header), and shuffle the rows (otherwise\n",
    "    # all examples of a particular class will be in sequential order)\n",
    "    rows = open(csvPath).read().strip().split(\"\\n\")[1:]\n",
    "    random.shuffle(rows)\n",
    "    \n",
    "    # loop over the rows of the CSV file\n",
    "    for (i, row) in enumerate(rows):\n",
    "        # check to see if we should show a status update\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {} total images\".format(i))\n",
    "        # split the row into components and then grab the class ID\n",
    "        # and image path\n",
    "        (label, imagePath) = row.strip().split(\",\")[-2:]\n",
    "        # derive the full path to the image file and load it\n",
    "        imagePath = os.path.sep.join([basePath, imagePath])\n",
    "        image = io.imread(imagePath)\n",
    "        \n",
    "        # resize the image to be 32x32 pixels, ignoring aspect ratio,\n",
    "        # and then perform Contrast Limited Adaptive Histogram\n",
    "        # Equalization (CLAHE)\n",
    "        image = transform.resize(image, (32, 32))\n",
    "        image = exposure.equalize_adapthist(image, clip_limit=0.1)\n",
    "        # update the list of data and labels, respectively\n",
    "        data.append(image)\n",
    "        labels.append(int(label))\n",
    "    # convert the data and labels to NumPy arrays\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    # return a tuple of the data and labels\n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "INIT_LR = 1e-3\n",
    "BS = 64\n",
    "# load the label names\n",
    "labelNames = open(\"signnames.csv\").read().strip().split(\"\\n\")[1:]\n",
    "labelNames = [l.split(\",\")[1] for l in labelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading training and testing data...\n",
      "[INFO] processed 1000 total images\n",
      "[INFO] processed 2000 total images\n",
      "[INFO] processed 3000 total images\n",
      "[INFO] processed 4000 total images\n",
      "[INFO] processed 5000 total images\n",
      "[INFO] processed 6000 total images\n",
      "[INFO] processed 7000 total images\n",
      "[INFO] processed 8000 total images\n",
      "[INFO] processed 9000 total images\n",
      "[INFO] processed 10000 total images\n",
      "[INFO] processed 11000 total images\n",
      "[INFO] processed 12000 total images\n",
      "[INFO] processed 13000 total images\n",
      "[INFO] processed 14000 total images\n",
      "[INFO] processed 15000 total images\n",
      "[INFO] processed 16000 total images\n",
      "[INFO] processed 17000 total images\n",
      "[INFO] processed 18000 total images\n",
      "[INFO] processed 19000 total images\n",
      "[INFO] processed 20000 total images\n",
      "[INFO] processed 21000 total images\n",
      "[INFO] processed 22000 total images\n",
      "[INFO] processed 23000 total images\n",
      "[INFO] processed 24000 total images\n",
      "[INFO] processed 25000 total images\n",
      "[INFO] processed 26000 total images\n",
      "[INFO] processed 27000 total images\n",
      "[INFO] processed 28000 total images\n",
      "[INFO] processed 29000 total images\n",
      "[INFO] processed 30000 total images\n",
      "[INFO] processed 31000 total images\n",
      "[INFO] processed 32000 total images\n",
      "[INFO] processed 33000 total images\n",
      "[INFO] processed 34000 total images\n",
      "[INFO] processed 35000 total images\n",
      "[INFO] processed 36000 total images\n",
      "[INFO] processed 37000 total images\n",
      "[INFO] processed 38000 total images\n",
      "[INFO] processed 39000 total images\n",
      "[INFO] processed 1000 total images\n",
      "[INFO] processed 2000 total images\n",
      "[INFO] processed 3000 total images\n",
      "[INFO] processed 4000 total images\n",
      "[INFO] processed 5000 total images\n",
      "[INFO] processed 6000 total images\n",
      "[INFO] processed 7000 total images\n",
      "[INFO] processed 8000 total images\n",
      "[INFO] processed 9000 total images\n",
      "[INFO] processed 10000 total images\n",
      "[INFO] processed 11000 total images\n",
      "[INFO] processed 12000 total images\n"
     ]
    }
   ],
   "source": [
    "# derive the path to the training and testing CSV files\n",
    "trainPath = os.path.sep.join([base_path, \"Train.csv\"])\n",
    "testPath = os.path.sep.join([base_path, \"Test.csv\"])\n",
    "# load the training and testing data\n",
    "print(\"[INFO] loading training and testing data...\")\n",
    "(trainX, trainY) = load_split(base_path, trainPath)\n",
    "(testX, testY) = load_split(base_path, testPath)\n",
    "# scale data to the range of [0, 1]\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "# one-hot encode the training and testing labels\n",
    "numLabels = len(np.unique(trainY))\n",
    "trainY = to_categorical(trainY, numLabels)\n",
    "testY = to_categorical(testY, numLabels)\n",
    "# calculate the total number of images in each class and\n",
    "# initialize a dictionary to store the class weights\n",
    "classTotals = trainY.sum(axis=0)\n",
    "classWeight = dict()\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "\tclassWeight[i] = classTotals.max() / classTotals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/30\n",
      "612/612 [==============================] - 55s 83ms/step - loss: 7.6169 - accuracy: 0.1757 - val_loss: 3.2801 - val_accuracy: 0.1289\n",
      "Epoch 2/30\n",
      "612/612 [==============================] - 54s 89ms/step - loss: 4.3558 - accuracy: 0.4150 - val_loss: 1.2384 - val_accuracy: 0.5579\n",
      "Epoch 3/30\n",
      "612/612 [==============================] - 55s 89ms/step - loss: 3.0259 - accuracy: 0.5527 - val_loss: 0.9494 - val_accuracy: 0.6818\n",
      "Epoch 4/30\n",
      "612/612 [==============================] - 58s 94ms/step - loss: 2.2663 - accuracy: 0.6431 - val_loss: 4.9304 - val_accuracy: 0.1247\n",
      "Epoch 5/30\n",
      "612/612 [==============================] - 59s 96ms/step - loss: 1.8118 - accuracy: 0.7125 - val_loss: 0.6485 - val_accuracy: 0.7788\n",
      "Epoch 6/30\n",
      "612/612 [==============================] - 63s 103ms/step - loss: 1.4740 - accuracy: 0.7621 - val_loss: 0.6325 - val_accuracy: 0.7913\n",
      "Epoch 7/30\n",
      "612/612 [==============================] - 61s 100ms/step - loss: 1.2483 - accuracy: 0.7985 - val_loss: 0.5260 - val_accuracy: 0.8220\n",
      "Epoch 8/30\n",
      "612/612 [==============================] - 60s 98ms/step - loss: 1.0910 - accuracy: 0.8254 - val_loss: 0.3461 - val_accuracy: 0.8876\n",
      "Epoch 9/30\n",
      "612/612 [==============================] - 63s 103ms/step - loss: 0.9624 - accuracy: 0.8419 - val_loss: 4.2276 - val_accuracy: 0.2496\n",
      "Epoch 10/30\n",
      "612/612 [==============================] - 55s 89ms/step - loss: 0.8670 - accuracy: 0.8583 - val_loss: 0.4408 - val_accuracy: 0.8603\n",
      "Epoch 11/30\n",
      "612/612 [==============================] - 54s 89ms/step - loss: 0.7682 - accuracy: 0.8747 - val_loss: 0.3500 - val_accuracy: 0.8842\n",
      "Epoch 12/30\n",
      "612/612 [==============================] - 68s 111ms/step - loss: 0.7265 - accuracy: 0.8822 - val_loss: 0.2710 - val_accuracy: 0.9121\n",
      "Epoch 13/30\n",
      "612/612 [==============================] - 60s 98ms/step - loss: 0.6566 - accuracy: 0.8955 - val_loss: 0.2431 - val_accuracy: 0.9197\n",
      "Epoch 14/30\n",
      "612/612 [==============================] - 53s 87ms/step - loss: 0.6186 - accuracy: 0.9008 - val_loss: 0.3302 - val_accuracy: 0.8969\n",
      "Epoch 15/30\n",
      "612/612 [==============================] - 55s 90ms/step - loss: 0.6089 - accuracy: 0.9028 - val_loss: 0.6003 - val_accuracy: 0.8349\n",
      "Epoch 16/30\n",
      "612/612 [==============================] - 53s 87ms/step - loss: 0.5538 - accuracy: 0.9107 - val_loss: 0.2666 - val_accuracy: 0.9138\n",
      "Epoch 17/30\n",
      "612/612 [==============================] - 54s 89ms/step - loss: 0.5143 - accuracy: 0.9168 - val_loss: 0.3235 - val_accuracy: 0.9046\n",
      "Epoch 18/30\n",
      "612/612 [==============================] - 54s 88ms/step - loss: 0.4764 - accuracy: 0.9228 - val_loss: 0.2163 - val_accuracy: 0.9314\n",
      "Epoch 19/30\n",
      "612/612 [==============================] - 55s 89ms/step - loss: 0.4755 - accuracy: 0.9226 - val_loss: 0.2780 - val_accuracy: 0.9168\n",
      "Epoch 20/30\n",
      "612/612 [==============================] - 58s 94ms/step - loss: 0.4475 - accuracy: 0.9285 - val_loss: 0.6725 - val_accuracy: 0.8054\n",
      "Epoch 21/30\n",
      "612/612 [==============================] - 55s 90ms/step - loss: 0.4487 - accuracy: 0.9291 - val_loss: 0.9235 - val_accuracy: 0.7866\n",
      "Epoch 22/30\n",
      "612/612 [==============================] - 56s 92ms/step - loss: 0.4027 - accuracy: 0.9349 - val_loss: 0.2623 - val_accuracy: 0.9229\n",
      "Epoch 23/30\n",
      "612/612 [==============================] - 55s 90ms/step - loss: 0.4057 - accuracy: 0.9356 - val_loss: 0.4102 - val_accuracy: 0.8776\n"
     ]
    }
   ],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "\trotation_range=10,\n",
    "\tzoom_range=0.15,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=False,\n",
    "\tvertical_flip=False,\n",
    "\tfill_mode=\"nearest\")\n",
    "\n",
    "# initialize the optimizer and compile the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / (NUM_EPOCHS * 0.5))\n",
    "model = TrafficSignNet.build(width=32, height=32, depth=3,\n",
    "\tclasses=numLabels)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "#early callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            patience=5,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=trainX.shape[0] // BS,\n",
    "\tepochs=NUM_EPOCHS,\n",
    "\tclass_weight=classWeight,\n",
    "    callbacks=[callback],\n",
    "\tverbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "                                                    precision    recall  f1-score   support\n",
      "\n",
      "                              Speed limit (20km/h)       0.62      0.98      0.76        60\n",
      "                              Speed limit (30km/h)       0.84      0.98      0.90       720\n",
      "                              Speed limit (50km/h)       0.87      0.88      0.88       750\n",
      "                              Speed limit (60km/h)       0.94      0.90      0.92       450\n",
      "                              Speed limit (70km/h)       0.99      0.88      0.93       660\n",
      "                              Speed limit (80km/h)       0.89      0.83      0.86       630\n",
      "                       End of speed limit (80km/h)       0.97      0.95      0.96       150\n",
      "                             Speed limit (100km/h)       0.90      0.92      0.91       450\n",
      "                             Speed limit (120km/h)       0.97      0.87      0.91       450\n",
      "                                        No passing       0.98      0.97      0.98       480\n",
      "      No passing for vehicles over 3.5 metric tons       1.00      0.96      0.98       660\n",
      "             Right-of-way at the next intersection       0.92      0.95      0.94       420\n",
      "                                     Priority road       0.97      0.98      0.98       690\n",
      "                                             Yield       1.00      0.99      1.00       720\n",
      "                                              Stop       0.98      1.00      0.99       270\n",
      "                                       No vehicles       0.91      1.00      0.95       210\n",
      "          Vehicles over 3.5 metric tons prohibited       0.99      0.99      0.99       150\n",
      "                                          No entry       1.00      0.97      0.99       360\n",
      "                                   General caution       0.97      0.82      0.89       390\n",
      "                       Dangerous curve to the left       0.97      0.95      0.96        60\n",
      "                      Dangerous curve to the right       0.83      0.98      0.90        90\n",
      "                                      Double curve       0.80      0.68      0.73        90\n",
      "                                        Bumpy road       0.98      0.84      0.91       120\n",
      "                                     Slippery road       0.87      0.97      0.92       150\n",
      "                         Road narrows on the right       0.79      0.96      0.86        90\n",
      "                                         Road work       0.94      0.97      0.95       480\n",
      "                                   Traffic signals       0.93      0.96      0.94       180\n",
      "                                       Pedestrians       0.60      1.00      0.75        60\n",
      "                                 Children crossing       0.90      0.94      0.92       150\n",
      "                                 Bicycles crossing       0.88      1.00      0.94        90\n",
      "                                Beware of ice/snow       0.84      0.65      0.73       150\n",
      "                             Wild animals crossing       0.91      0.98      0.94       270\n",
      "               End of all speed and passing limits       0.92      1.00      0.96        60\n",
      "                                  Turn right ahead       1.00      0.99      0.99       210\n",
      "                                   Turn left ahead       0.98      1.00      0.99       120\n",
      "                                        Ahead only       0.99      0.97      0.98       390\n",
      "                              Go straight or right       0.96      1.00      0.98       120\n",
      "                               Go straight or left       0.97      0.98      0.98        60\n",
      "                                        Keep right       1.00      0.91      0.95       690\n",
      "                                         Keep left       1.00      0.96      0.98        90\n",
      "                              Roundabout mandatory       0.61      0.91      0.73        90\n",
      "                                 End of no passing       0.74      1.00      0.85        60\n",
      "End of no passing by vehicles over 3.5 metric tons       0.92      0.64      0.76        90\n",
      "\n",
      "                                          accuracy                           0.93     12630\n",
      "                                         macro avg       0.91      0.93      0.91     12630\n",
      "                                      weighted avg       0.94      0.93      0.93     12630\n",
      "\n",
      "[INFO] serializing network to './trafficsignnet.model'...\n",
      "INFO:tensorflow:Assets written to: ./trafficsignnet.model\\assets\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network #testing\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=labelNames))\n",
    "# save the network to disk\n",
    "print(\"[INFO] serializing network to '{}'...\".format(save_path))\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, callback.stopped_epoch+1)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1eb00ff78cf2>:1: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import load_model\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "from skimage import io\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import random\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "save_path = \"./trafficsignnet.model\"\n",
    "plot_path = \"./plot.png\"\n",
    "test_path = \"./Test/\"\n",
    "example_path = \"./examples\" #create examples directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] predicting...\n"
     ]
    }
   ],
   "source": [
    "# load the traffic sign recognizer model\n",
    "print(\"[INFO] loading model...\")\n",
    "model = load_model(save_path)\n",
    "# load the label names\n",
    "labelNames = open(\"signnames.csv\").read().strip().split(\"\\n\")[1:]\n",
    "labelNames = [l.split(\",\")[1] for l in labelNames]\n",
    "# grab the paths to the input images, shuffle them, and grab a sample\n",
    "print(\"[INFO] predicting...\")\n",
    "imagePaths = list(paths.list_images(test_path))\n",
    "random.shuffle(imagePaths)\n",
    "imagePaths = imagePaths[:25]\n",
    "example_images = []\n",
    "# loop over the image paths\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # load the image, resize it to 32x32 pixels, and then apply\n",
    "    # Contrast Limited Adaptive Histogram Equalization (CLAHE),\n",
    "    # just like we did during training\n",
    "    image = io.imread(imagePath)\n",
    "    image = transform.resize(image, (32, 32))\n",
    "    image = exposure.equalize_adapthist(image, clip_limit=0.1)\n",
    "    # preprocess the image by scaling it to the range [0, 1]\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # make predictions using the traffic sign recognizer CNN\n",
    "    preds = model.predict(image)\n",
    "    j = preds.argmax(axis=1)[0]\n",
    "    label = labelNames[j]\n",
    "    # load the image using OpenCV, resize it, and draw the label\n",
    "    # on it\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=128)\n",
    "    cv2.putText(image, label, (5, 15), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.45, (0, 0, 255), 2)\n",
    "    # save the image to disk\n",
    "    p = os.path.sep.join([example_path, \"{}.png\".format(i)])\n",
    "    example_images.append(image)\n",
    "    cv2.imwrite(p, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#either refer examples folder or run the below command\n",
    "#press esc to exit window\n",
    "\n",
    "for img in example_images:\n",
    "    cv2.imshow(\"Image\",img)\n",
    "    k = cv2.waitKey(0)\n",
    "    if k==27:\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
